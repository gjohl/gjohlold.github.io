[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Projects"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "Data science, AI and software projects."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Home",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nFast AI Notes\n\n\n\n\n\n\nAI\n\n\n\nNotes from Fast AI Practical Deep Learning for Coders Part 1\n\n\n\n\n\nAug 30, 2023\n\n\n20 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/fastai/index.html",
    "href": "posts/fastai/index.html",
    "title": "Fast AI Notes",
    "section": "",
    "text": "Notes from Fast AI Practical Deep Learning for Coders Part 1.\nhttps://course.fast.ai/ https://github.com/fastai/fastbook/tree/master\n\n\n\nHomework task:\nTrain an image classifier https://github.com/gjohl/ml-practice/blob/master/ml-practice/notebooks/fastai/1_image_classifier.ipynb\n\nEthics course https://ethics.fast.ai/\nResearch on education: Coloured cups - green, amber, red - Meta learning by Radek Osmulski Mathematician’s Lament by Paul Lockhart Making Learning Whole by David Perkins\nBefore deep learning, the approach to machine learning was to enlist many domain experts to handcraft features and feed this into a constrained linear model (e.g. ridge regression). This is time-consuming, expensive and requires many domain experts. Neural networks learn these features. Looking inside a CNN, for example, shows that these learned features match interpretable features that an expert might handcraft.\nFor image classifiers, you don’t need particularly large images as inputs. GPUs are so quick now that if you use large images, most of the time is spent on opening the file rather than computations. So often we resize images down to 400x400 pixels or smaller.\nFor most use cases, there are pre-trained models and sensible default values that we can use. In practice, most of the time is spent on the input layer and output layer. For most models the middle layers are identical.\nData blocks structure the input to learners. DataBlock class: - blocks determines the input and output type as a tuple. For multi-target classification this tuple can be arbitrary length. - get_items - function that returns a list of all the inputs - splitter - how to split the training/validation set - get_y - function that returns the label of a given input image - item_tfms - what transforms to apply to the inputs before training, e.g. resize - dataloaders - method that parallelises loading the data.\nA learner combines the model (e.g. resnet or something from timm library) and the data to run that model on (the dataloaders from the DataBlock). - fine_tune method starts with a pretrained model weights rather than randomised weights, and only needs to learn the differences between your data and the original model.\nOther image problems that can utilise deep learning - Image classification - Image segmentation\nOther problem types use the same process, just with different DataBlock blocks types and the rest is the same. For example, tabular data, collaborative filtering.\nRISE is a jupyter notebook extensions to turn notebooks into slides. Jeremy uses notebooks for: source code, book, blogging, CI/CD.\nTraditional computer programs are essentially:\ninputs ---&gt; program ---&gt; results\nDeep learning models are:\ninputs ---&gt; model ---&gt; results ---&gt; loss\n            ^                         |\nweights ----|                         |\n ^                                    |\n |--------------(update)--------------|\n\n\n\n\nHomework task:\nDeploy a model to Huggingface Spaces https://huggingface.co/spaces/GurpreetJohl/binary_image_classifier_vw_rr\nDeploy a model to a Github Pages website https://github.com/gjohl/vw_classifier\n\nIt can be useful to train a model on the data BEFORE you clean it - Counterintuitive! - The confusion matrix output of the learner gives you a good intuition about which classifications are hard - plot_top_losses shows which examples were hardest for the model to classify. This can find (1) when the model is correct but not confident, and (2) when the model was confident but incorrect - ImageClassifierCleaner shows the examples in the training and validation set ordered by loss, so we can choose to keep, reclassify or remove them\nFor image resizing, random resize crop can often be more effective. - Squishing can result in weird, unrealistic images - Padding or mirroing can add false information that the model will erroneously learn - Random crops give different sections of the image which acts as a form of data augmentation. - aug_transforms can be use for more sophisticated data augmentation like warping and recoloring images.\nA website for quizzes based on the book: www.aiquizzes.com\nHugging face spaces hosts models with a choice of pre-canned interfaces (Gradio in the example in the lecture) to quickly deploy a model to the public. Streamlit is an alternative to Gradio that is more flexible. https://huggingface.co/spaces\nSaving a model Once you are happy with the model you’ve trained, you can pickle the learner object and save it.\nlearn.export('model.pkl')\nThen you can add the saved model to the hugging face space. To use the model to make predictions Any external functions you used to create the model will need to be instantiated too.\nlearn = load_learner('model.pkl')\nlearn.predict(image)\nGradio requires a dict of classes as keys and probabilities (as floats not tensors) as the values. To go from the Gradio prototype to a production app, you can view the Gradio API from the huggingface space which will show you the API. The API exposes an endpoint which you can then hit from your own frontend app.\nGithub pages is a free and simple way to host a public website. See this repo as an example of a minimal example html website which issues GET requests to the Gradio API https://github.com/fastai/tinypets\nTo convert a notebook to a python script, you can add #|export to the top of any cells to include in the script, then use:\nfrom nbdev.export import notebook2script\nnotebook2script('name_of_output_file.py')\nUse #|default_exp app in the first cell of the notebook to set the default name of the exported python file.\nHow to choose the number of epochs to train for? Whenever it is “good enough” for your use case. If you need to train for longer, you may need to use data augmentation to prevent overfitting. Keep an eye on the validation error to check overfitting.\n\n\n\n\nHomework task: Recreate the spreadsheet to train a linear model and a neural network from scratch https://docs.google.com/spreadsheets/d/1hma4bTEFuiS483djqE5dPoLlbsSQOTioqMzsesZGUGI/edit?usp=sharing\n\nOptions for cloud environments: Kaggle, Colab, Paperspace\nComparison of performance vs training time for different image models: https://www.kaggle.com/code/jhoward/which-image-models-are-best/ Resnet and convnext are generally good to start with. The best practice is to start with a “good enough” model with a quick training time, so you can iterate quickly. Then as you get further on with your research and need a better model, you can move to a slower, more accurate model. The criteria we generally care about are: 1. How fast are they 2. How much memory do they use 3. How accurate are they\nA learner object contains the pre-processing steps and the model itself.\nFit a quadratic function How do we fit a function to data? An ML model is just a function fitted to data. Deep learning is just fitting an infinitely flexible model to data. In this notebook there is an interactive cell to fit a quadratic function to some noisy data: y = ax^2 +bx + c We can vary a, b and c to get a better fit by eye. We can make this more rigorous by defining a loss function to quantify how good the fit is. In this case, use the mean absolute error mae = mean(abs(actual - preds)) We can then use stochastic gradient descent to autome the tweaking of a, b and c to minimise the loss.\nWe can store the parameters as a tensor, then pytorch will calculate gradients of the loss function based on that tensor.\nabc = abc = torch.tensor([1.1,1.1,1.1]) \nabc.requires_grad_()  # Modifies abc in place so that it will include gradient calculations on anything which uses abc downstream\nloss = quad_mae(abc)  # loss uses abc so we will get the gradient of loss too\nloss.backward()  # Back-propagate the gradients\nabc.grad  # Returns a tensor of the loss gradients\nWe can then take a “small step” in the direction that will decrease the gradient. The size of the step should be proportional to the size of the gradient. We define a learning rate hyperparameter to determine how much to scale the gradients by.\nlearning_rate = 0.01\nabc -= abc.grad * learning_rate\nloss = quad_mae(abc)  # The loss should now be smaller\nWe can repeat this process to take multiple steps to minimise the gradient.\nfor step in range(10):\n    loss = quad_mae(abc)\n    loss.backward()\n    with torch.no_grad():\n        abc -= abc.grad * learning_rate\n    print(f'step={step}; loss={loss:.2f}')\nThe learning rate should decrease as we get closer to the minimum to ensure we don’t overshoot the minimum and increase the loss. A learning rate schedule can be specified to do this.\nFit a deep learning model For deep learning, the premise is the same but instead of quadratic functions, we fit ReLUs and other non-linear functions. Universal approximation theorem states that this is infinitely expressive if enough ReLUs (or other non-linear units) are combined. This means we can learn any computable function.\nA ReLU is essentially a linear function with the negative values clipped to 0\ndef relu(m,c,x):\n    y = m * x + c\n    return np.clip(y, 0.)\nThis is all that deep learning is! All we need is: 1. A model - a bunch of ReLUs combined will be flexible 2. A loss function - mean absolute error between the actual data values and the values predicted by the model 3. An optimiser - stochastic gradient descent can start from random weights to incrementally improve the loss until we get a “good enough” fit\nWe just need enough time and data. There are a few hacks to decrease the time and data required: - Data augmentation - Running on GPUs to parallelise matrix multiplications - Convolutions to skip over values to reduce the number of matrix multiplications required - Transfer learning - initialise with parameters from another pre-trained model instead of random weights.\nThis spreadsheet is a worked example of manually training a multivariate linear model, then extending that to a neural network summing two ReLUs.\n\n\n\n\nHomework task:\nKaggle NLP pattern similarity notebook https://www.kaggle.com/code/gurpreetjohl/getting-started-with-nlp-for-absolute-beginners/edit\n\nNLP applications: categorising documents, translation, text generation.\nUsing Huggingface transformers library for this lesson. It is now incorporated into the fastai library.\nULMFit is an algorithm which uses fine-tuning, in this example to train a positve/negative sentiment classifier in 3 steps: 1. Train an RNN on wikipedia to predict the next word. No labels required. 2. Fine-tune this for IMDb reviews to predict the next word of a movie review. Still no labels required. 3. Fine-tune this to classify the sentiment.\nTransformers have overtaken ULMFit as the state-of-the-art.\nLooking “inside” a CNN, the first layer contains elementary detectors like edge detectors, blob detectors, gradient detectors etc. These get combined in non-linear ways to make increasingly complex detectors. Layer 2 might combine vertical and horizontal edge detectors into a corner detector. By the later layers, it is detecting rich features like lizard eyes, dog faces etc. See: https://arxiv.org/abs/1311.2901\nFor the fine-tuning process, the earlier layers are unlikely to need changing because they are more general. So we only need to fine-tune (AKA re-train) the later layers.\nKaggle competition walkthrough\nhttps://www.kaggle.com/code/gurpreetjohl/getting-started-with-nlp-for-absolute-beginners/edit\nReshape the input to fit a standard NLP task - We want to learn the similarity between two fields and are provided with similarity scores. - We concat the fields of interest (with identifiers in between). The identifiers themselves are not important, they just need to be consistent. - The NLP model is then a supervised regression task to predict the score given the concatendated string.\ndf['input'] = 'TEXT1: ' + df.context + '; TEXT2: ' + df.target + '; ANC1: ' + df.anchor\nTokenization: Split the text into tokens (words). Tokens are, broadly speaking, words. There are some caveats to that, as some languages like Chinese don’t fit nicely into that model. We don’t want the vocabulary to be too big. In practice, we tokenize into subwords.\nNumericalization: Map each unique token to a number. One-hot encoding.\nThe choice of tokenization and numericalization depends on the model you use. Whoever trained the model chose a convention for tokenizing. We need to be consistent with that if we want the model to work correctly.\nModels: The Huggingface model hub contains thousands of pretrained models https://huggingface.co/models For NLP tasks, it is useful to choose a model that was trained on a similar corpus, so you can search the model hub. In this case, we search for “patent”.\nSome models are general purpose, e.g. deberta-v3 used in the lesson.\nULMFit handles large documents better as it can split up the document. Transformer approaches require loading the whole document into GPU memory, so struggle for larger documents.\nOverfitting: If a model is too simple (i.e. not flexible enough) then it cannot fit the data and be biased. Underfitting.\nIf the model fits the data points too closely, it is overfitting.\nA good validation set, and monitoring validation error rather than training error as a metric, is key to avoiding overfitting. https://www.fast.ai/posts/2017-11-13-validation-sets.html\nOften people will default to using a random train/test split (this is what scikit-learn uses). This is a BAD idea very often. For time-series data, it’s easier to infer gaps than it is to predict a block in the future. The latter is the more common task but a random split simulates the former, giving unrealistically good performance. For image data, there may be people, boats, etc that are in the training set but not the test set. By failing to have new people in the validation set, the model can learn things about specific people/boats that it can’t rely on in practice.\nMetrics vs loss functions: Metrics are things that are human-understandable. Loss functions should be smooth and differentiable to aid in training.\nThese can sometimes be the same thing, but not in general. For example, accuracy is a good metric in image classification. We could tweak the weights in such a way that it improves the model slightly, but not so much that it now correctly classifies a previously incorrect image. This means the metric function is bumpy, therefore a bad loss function. https://www.fast.ai/posts/2019-09-24-metrics.html\nAI can be particularly dangerous at confirming systematic biases, because it is so good at optimising metrics, so it will conform to any biases present in the training data. MAking decisions based on the model then reinforces those biases. - Goodhart’s law applies: If a metric becomes a target it’s no longer a good metric\nCorrelations The best way to understand a metric is not to look at the mathematical formular, but to plot some data for which the metric is high, medium and low, then see what that tells you.\nAfter that, look at the equation to see if your intuition matches the logic.\nChoosing a learning rate Fast AI has a function to find a good starting point. Otherwise, pick a small value and keep doubling it until it falls apart.\n\n\n\n\nHomework task: Recreate the Jupyter notebook to train a linear model and a neural network from scratch\nThen use the fast AI library to recreate the same model in less code\nRead the numpy broadcasting rules\nRead through Titanic notebooks\n\nTrain a linear model from scratch in Python using on the Titanic dataset. Then train a neural network from scratch in a similar way. This is an extension of the spreadsheet approach to make a neural network from scratch in lesson 3.\nImputing missing values Never throw away data. An easy way of imputing missing values is to fill them with the mode. This is good enough for a first pass at creating a model.\nScaling values Numeric values that can grow exponentially like prices or population sizes often have long-tailed distributions. An easy way to scale is to take log(x+1). The +1 is just to avoid taking log of 0.\nCategorical variables One-hot encode any categorical variables. We should include an “other” category for each in case the validation or test set contains a category we didn’t encounter in the training set. If there are categories with ony a small number of observations we can group them into an “other” category too.\nBroadcasting Broadcasting arrays together avoids boilerplate code to make dimensions match. https://numpy.org/doc/stable/user/basics.broadcasting.html https://tryapl.org/\nSigmoid final layer For a binary classification model, the outputs should be between 0 and 1. If you train a linear model, it might result in negative values or values &gt;1. This means we can improve the loss by just clipping to ensure they stay between 0 and 1. This is the idea behind the sigmoid function for output layers: smaller values will tend to 0 and larger values will tend to 1. In general, for any binary classification model, we should always have a sigmoid as the final layer. If the model isn’t training well, it’s worth checking that the final activation function is a sigmoid.\nFunction: y = 1 / (1+e^-x))\nFocus on input and output layers In general, the middle layers of a neural network are similar between different problems. The input layer will depend on the data for our specific problem. The output will depend on the target for our specific problem. So we spend most of our time thinking about the correct input and output layers, and the hidden layers are less important.\nUsing a framework When creating the models from scratch, there was a lot of boilerplate code to: - Impute missing values using the “obvious” method (fill with mode) - Normalise continuous variables to be between 0 and 1 - One hot encode categorical variables - Repeat all of these steps in the same order for the test set\nThe benefits of using a framework like fastai: - Less boilerplate, so the obvious things are done automatically unless you say otherwise - Repeating all of the feature engineering steps on the output is trivial with DataLoaders\nEnsemble Creating independent models and taking the mean of their predictions improves the accuracy. There are a few different approaches to ensembling a categorical variable: 1. Take the mean of the predictions (binary 0 or 1) 2. Take the mean of the probabilities (continuous between 0 and 1) then threshold the result 3. Take the mode of the predictions (binary 0 or 1)\nIn general the mean approaches work better but there’s no rule as to why, so try all of them.\n\n\n\nIt’s worth starting with a random forest as a baseline model because “it’s very hard to screw up”. Decision trees only consider the ordering of data, so are not sensitive to outliers, skewed distributions, dummy variables etc.\nA random forest is an ensemble of trees. A tree is an ensemble of binary splits. binary split -&gt; tree -&gt; forest\nBinary split Pick a column of the data set and split the rows into two groups. Predict the outcome based just on that. For example, in the titanic dataset, pick the Sex column. This splits into male vs female. If we predict that all female passengers survived and all male passengers died, that’s a reasonably accurate prediction.\nA model which iterates through the variables and finds the best binary split is called a “OneR” or one rule model. This was actually the state-of-the-art in the 90s and performs reasonably well across a lot of problems.\nDecision tree This extends the “OneR” idea to two or more splits. E.g. if we first split by sex, then find the next best variable to split on to create a two layer tree.\nLeaf nodes are the number of terminating nodes in the tree. OneR creates 2 leaf nodes. If we split one side again, we’ll have 3 leaf nodes. If we split the other side, to create a balanced two layer tree, we’ll have 4 leaf nodes.\nGini is another measure of inequality, similar to the score used in the notebook to quantify how good a split was. Intuitively, this measures the likelihood that if you picked two samples from a group, how likely is it they’d be the same every time. If the group is all the same, the probability is 1. If every item is different, the probability is 0.\nRandom forest The idea behind bagging is that if we have many unbiased, uncorrelated models, we can average their predictions to get an aggregate model that is better than any of them. Each individual model will overfit, so either be too high or too low for a given point, a positive or negative error term respectively. By combining multiple uncorrelated models, the error will average to 0.\nAn easy way to get many uncorrelated models is to only use a random subset of the data each time. Then build a decision tree for each subset. This is the idea behind random forests.\nThe error decreases with the number of tree, with diminishing returns. Jeremy’s rule of thumb: improvements level off after about 30 and he doesn’t often use &gt;100.\nFeature importance A nice side effect of using decision trees is that we get feature importance plots for free.\nThe idea is for each split of a decision tree, we can track the column used to split and the amount that the gini decreased by. If we loop through the tree and accumulate the “gini reduction” for each column, we have a metric of how important that column was in splitting the dataset.\nThis makes random forests a useful first model when tackling a new big dataset, as it can give an insight into how useful each column is.\nA comment on explainability, regarding feature importance compared to SHAP, LIME etc. These explain the model, so to be useful the model needs to be accurate. If you use feature importance from a bad model then the columns it claims are important might not actually be. So the usefulness of explainability techniques boils down to what models can they explain and how accurate are those models.\nOut-of-bag error For each tree, we trained on a subset of the rows. We can then see how each one performed on the held out data; this is the out-of-bag error per tree. We can average this over all of the trees to get an overall OOB error for the forest.\nPartial dependence plots These are not specific to random forests and can be applied to any ML model including deep neural networks.\nIf we want to know how an independent variable affects the dependent variable, a naive approach would be to just plot them. But this could include a confounding variable. For example, the price of bulldozers increases over time, but driven by the presence of air conditioning which also increased over time.\nA partial dependence plot takes each row in turn and sets the column(s) of interest to the first value, say, year=1950. Then we predict the target variable using our model. Then repeat this for year=1951, 1952, etc. We can then average the target variable per year to get a view of how it depends on the independent variable, all else being equal.\nGradient boosting forests We make multiple trees, but instead of fitting all to different data subsets, we fit to residuals. So we fit a very small tree (OneR even) to the data to get a first prediction. Then we calculate the error term. Then we fit another tree to predict the error term. Then calculate the second order error term. Then fit a tree to this, etc.\nThen our prediction is the SUM of these trees (rather than the average like with a random forest). This is “boosting”; calculating an error term then fitting another model to it. Contrast this with “bagging” which was when we calculate multiple models to different subsets of data and average them.\nKaggle iterations The focus should be: 1. Create an effective validation set 2. Iterate quickly to find changes which improve the validation set\nTrain a simple model straight away, get a result and submit it. You can iterate and improve later.\nRules of thumb on model selection by problem type: - For computer vision use CNNs, fine-tune a pre-trained model. See comparison of pre-trained models here - If unsure which model to choose, start with (small) convnext. - Different models can have big differences in accuracy so try a few small models from different families. - Once you are happy with the model, try a size up in that family. - For tabular data, random forests will give a reasonably good results. - GBMs will give a better result eventually, but with more effort required to get a good model. - Worth running a hyperparameter grid search for GBM because it’s fiddly.\nRules of thumb on hardware: - You generally want ~8 physical CPUs per GPU - If model training is CPU bound, it can help to resize images to be smaller. The file I/O on the CPU may be taking too long. - If model training is CPU bound and GPU usage is low, you might as well go up to a “better” (i.e. bigger) model with no increase of overall execution time.\nData augmentation, in particular test-time augmentation (TTA), can improve results.\nResizing images to a square is a good, easy compromise to accomodate many different iamge sizes , orientations and aspect ratios. A more involved approach that may improve results is to batch images together and resize them to the median rectangle size.\n\n\n\n\n\n\n\nBagging predictors https://link.springer.com/article/10.1007/BF00058655\nStatistical Modeling: The Two Cultures https://www.semanticscholar.org/paper/Statistical-Modeling%3A-The-Two-Cultures-(with-and-a-Breiman/e5df6bc6da5653ad98e754b08f63326c2e52b372\nComparison of vision models https://www.kaggle.com/code/jhoward/the-best-vision-models-for-fine-tuning"
  },
  {
    "objectID": "posts/fastai/index.html#introduction-to-image-classification-models",
    "href": "posts/fastai/index.html#introduction-to-image-classification-models",
    "title": "Fast AI Notes",
    "section": "",
    "text": "Homework task:\nTrain an image classifier https://github.com/gjohl/ml-practice/blob/master/ml-practice/notebooks/fastai/1_image_classifier.ipynb\n\nEthics course https://ethics.fast.ai/\nResearch on education: Coloured cups - green, amber, red - Meta learning by Radek Osmulski Mathematician’s Lament by Paul Lockhart Making Learning Whole by David Perkins\nBefore deep learning, the approach to machine learning was to enlist many domain experts to handcraft features and feed this into a constrained linear model (e.g. ridge regression). This is time-consuming, expensive and requires many domain experts. Neural networks learn these features. Looking inside a CNN, for example, shows that these learned features match interpretable features that an expert might handcraft.\nFor image classifiers, you don’t need particularly large images as inputs. GPUs are so quick now that if you use large images, most of the time is spent on opening the file rather than computations. So often we resize images down to 400x400 pixels or smaller.\nFor most use cases, there are pre-trained models and sensible default values that we can use. In practice, most of the time is spent on the input layer and output layer. For most models the middle layers are identical.\nData blocks structure the input to learners. DataBlock class: - blocks determines the input and output type as a tuple. For multi-target classification this tuple can be arbitrary length. - get_items - function that returns a list of all the inputs - splitter - how to split the training/validation set - get_y - function that returns the label of a given input image - item_tfms - what transforms to apply to the inputs before training, e.g. resize - dataloaders - method that parallelises loading the data.\nA learner combines the model (e.g. resnet or something from timm library) and the data to run that model on (the dataloaders from the DataBlock). - fine_tune method starts with a pretrained model weights rather than randomised weights, and only needs to learn the differences between your data and the original model.\nOther image problems that can utilise deep learning - Image classification - Image segmentation\nOther problem types use the same process, just with different DataBlock blocks types and the rest is the same. For example, tabular data, collaborative filtering.\nRISE is a jupyter notebook extensions to turn notebooks into slides. Jeremy uses notebooks for: source code, book, blogging, CI/CD.\nTraditional computer programs are essentially:\ninputs ---&gt; program ---&gt; results\nDeep learning models are:\ninputs ---&gt; model ---&gt; results ---&gt; loss\n            ^                         |\nweights ----|                         |\n ^                                    |\n |--------------(update)--------------|"
  },
  {
    "objectID": "posts/fastai/index.html#deployment",
    "href": "posts/fastai/index.html#deployment",
    "title": "Fast AI Notes",
    "section": "",
    "text": "Homework task:\nDeploy a model to Huggingface Spaces https://huggingface.co/spaces/GurpreetJohl/binary_image_classifier_vw_rr\nDeploy a model to a Github Pages website https://github.com/gjohl/vw_classifier\n\nIt can be useful to train a model on the data BEFORE you clean it - Counterintuitive! - The confusion matrix output of the learner gives you a good intuition about which classifications are hard - plot_top_losses shows which examples were hardest for the model to classify. This can find (1) when the model is correct but not confident, and (2) when the model was confident but incorrect - ImageClassifierCleaner shows the examples in the training and validation set ordered by loss, so we can choose to keep, reclassify or remove them\nFor image resizing, random resize crop can often be more effective. - Squishing can result in weird, unrealistic images - Padding or mirroing can add false information that the model will erroneously learn - Random crops give different sections of the image which acts as a form of data augmentation. - aug_transforms can be use for more sophisticated data augmentation like warping and recoloring images.\nA website for quizzes based on the book: www.aiquizzes.com\nHugging face spaces hosts models with a choice of pre-canned interfaces (Gradio in the example in the lecture) to quickly deploy a model to the public. Streamlit is an alternative to Gradio that is more flexible. https://huggingface.co/spaces\nSaving a model Once you are happy with the model you’ve trained, you can pickle the learner object and save it.\nlearn.export('model.pkl')\nThen you can add the saved model to the hugging face space. To use the model to make predictions Any external functions you used to create the model will need to be instantiated too.\nlearn = load_learner('model.pkl')\nlearn.predict(image)\nGradio requires a dict of classes as keys and probabilities (as floats not tensors) as the values. To go from the Gradio prototype to a production app, you can view the Gradio API from the huggingface space which will show you the API. The API exposes an endpoint which you can then hit from your own frontend app.\nGithub pages is a free and simple way to host a public website. See this repo as an example of a minimal example html website which issues GET requests to the Gradio API https://github.com/fastai/tinypets\nTo convert a notebook to a python script, you can add #|export to the top of any cells to include in the script, then use:\nfrom nbdev.export import notebook2script\nnotebook2script('name_of_output_file.py')\nUse #|default_exp app in the first cell of the notebook to set the default name of the exported python file.\nHow to choose the number of epochs to train for? Whenever it is “good enough” for your use case. If you need to train for longer, you may need to use data augmentation to prevent overfitting. Keep an eye on the validation error to check overfitting."
  },
  {
    "objectID": "posts/fastai/index.html#how-does-a-neural-net-learn",
    "href": "posts/fastai/index.html#how-does-a-neural-net-learn",
    "title": "Fast AI Notes",
    "section": "",
    "text": "Homework task: Recreate the spreadsheet to train a linear model and a neural network from scratch https://docs.google.com/spreadsheets/d/1hma4bTEFuiS483djqE5dPoLlbsSQOTioqMzsesZGUGI/edit?usp=sharing\n\nOptions for cloud environments: Kaggle, Colab, Paperspace\nComparison of performance vs training time for different image models: https://www.kaggle.com/code/jhoward/which-image-models-are-best/ Resnet and convnext are generally good to start with. The best practice is to start with a “good enough” model with a quick training time, so you can iterate quickly. Then as you get further on with your research and need a better model, you can move to a slower, more accurate model. The criteria we generally care about are: 1. How fast are they 2. How much memory do they use 3. How accurate are they\nA learner object contains the pre-processing steps and the model itself.\nFit a quadratic function How do we fit a function to data? An ML model is just a function fitted to data. Deep learning is just fitting an infinitely flexible model to data. In this notebook there is an interactive cell to fit a quadratic function to some noisy data: y = ax^2 +bx + c We can vary a, b and c to get a better fit by eye. We can make this more rigorous by defining a loss function to quantify how good the fit is. In this case, use the mean absolute error mae = mean(abs(actual - preds)) We can then use stochastic gradient descent to autome the tweaking of a, b and c to minimise the loss.\nWe can store the parameters as a tensor, then pytorch will calculate gradients of the loss function based on that tensor.\nabc = abc = torch.tensor([1.1,1.1,1.1]) \nabc.requires_grad_()  # Modifies abc in place so that it will include gradient calculations on anything which uses abc downstream\nloss = quad_mae(abc)  # loss uses abc so we will get the gradient of loss too\nloss.backward()  # Back-propagate the gradients\nabc.grad  # Returns a tensor of the loss gradients\nWe can then take a “small step” in the direction that will decrease the gradient. The size of the step should be proportional to the size of the gradient. We define a learning rate hyperparameter to determine how much to scale the gradients by.\nlearning_rate = 0.01\nabc -= abc.grad * learning_rate\nloss = quad_mae(abc)  # The loss should now be smaller\nWe can repeat this process to take multiple steps to minimise the gradient.\nfor step in range(10):\n    loss = quad_mae(abc)\n    loss.backward()\n    with torch.no_grad():\n        abc -= abc.grad * learning_rate\n    print(f'step={step}; loss={loss:.2f}')\nThe learning rate should decrease as we get closer to the minimum to ensure we don’t overshoot the minimum and increase the loss. A learning rate schedule can be specified to do this.\nFit a deep learning model For deep learning, the premise is the same but instead of quadratic functions, we fit ReLUs and other non-linear functions. Universal approximation theorem states that this is infinitely expressive if enough ReLUs (or other non-linear units) are combined. This means we can learn any computable function.\nA ReLU is essentially a linear function with the negative values clipped to 0\ndef relu(m,c,x):\n    y = m * x + c\n    return np.clip(y, 0.)\nThis is all that deep learning is! All we need is: 1. A model - a bunch of ReLUs combined will be flexible 2. A loss function - mean absolute error between the actual data values and the values predicted by the model 3. An optimiser - stochastic gradient descent can start from random weights to incrementally improve the loss until we get a “good enough” fit\nWe just need enough time and data. There are a few hacks to decrease the time and data required: - Data augmentation - Running on GPUs to parallelise matrix multiplications - Convolutions to skip over values to reduce the number of matrix multiplications required - Transfer learning - initialise with parameters from another pre-trained model instead of random weights.\nThis spreadsheet is a worked example of manually training a multivariate linear model, then extending that to a neural network summing two ReLUs."
  },
  {
    "objectID": "posts/fastai/index.html#natural-language-processing",
    "href": "posts/fastai/index.html#natural-language-processing",
    "title": "Fast AI Notes",
    "section": "",
    "text": "Homework task:\nKaggle NLP pattern similarity notebook https://www.kaggle.com/code/gurpreetjohl/getting-started-with-nlp-for-absolute-beginners/edit\n\nNLP applications: categorising documents, translation, text generation.\nUsing Huggingface transformers library for this lesson. It is now incorporated into the fastai library.\nULMFit is an algorithm which uses fine-tuning, in this example to train a positve/negative sentiment classifier in 3 steps: 1. Train an RNN on wikipedia to predict the next word. No labels required. 2. Fine-tune this for IMDb reviews to predict the next word of a movie review. Still no labels required. 3. Fine-tune this to classify the sentiment.\nTransformers have overtaken ULMFit as the state-of-the-art.\nLooking “inside” a CNN, the first layer contains elementary detectors like edge detectors, blob detectors, gradient detectors etc. These get combined in non-linear ways to make increasingly complex detectors. Layer 2 might combine vertical and horizontal edge detectors into a corner detector. By the later layers, it is detecting rich features like lizard eyes, dog faces etc. See: https://arxiv.org/abs/1311.2901\nFor the fine-tuning process, the earlier layers are unlikely to need changing because they are more general. So we only need to fine-tune (AKA re-train) the later layers.\nKaggle competition walkthrough\nhttps://www.kaggle.com/code/gurpreetjohl/getting-started-with-nlp-for-absolute-beginners/edit\nReshape the input to fit a standard NLP task - We want to learn the similarity between two fields and are provided with similarity scores. - We concat the fields of interest (with identifiers in between). The identifiers themselves are not important, they just need to be consistent. - The NLP model is then a supervised regression task to predict the score given the concatendated string.\ndf['input'] = 'TEXT1: ' + df.context + '; TEXT2: ' + df.target + '; ANC1: ' + df.anchor\nTokenization: Split the text into tokens (words). Tokens are, broadly speaking, words. There are some caveats to that, as some languages like Chinese don’t fit nicely into that model. We don’t want the vocabulary to be too big. In practice, we tokenize into subwords.\nNumericalization: Map each unique token to a number. One-hot encoding.\nThe choice of tokenization and numericalization depends on the model you use. Whoever trained the model chose a convention for tokenizing. We need to be consistent with that if we want the model to work correctly.\nModels: The Huggingface model hub contains thousands of pretrained models https://huggingface.co/models For NLP tasks, it is useful to choose a model that was trained on a similar corpus, so you can search the model hub. In this case, we search for “patent”.\nSome models are general purpose, e.g. deberta-v3 used in the lesson.\nULMFit handles large documents better as it can split up the document. Transformer approaches require loading the whole document into GPU memory, so struggle for larger documents.\nOverfitting: If a model is too simple (i.e. not flexible enough) then it cannot fit the data and be biased. Underfitting.\nIf the model fits the data points too closely, it is overfitting.\nA good validation set, and monitoring validation error rather than training error as a metric, is key to avoiding overfitting. https://www.fast.ai/posts/2017-11-13-validation-sets.html\nOften people will default to using a random train/test split (this is what scikit-learn uses). This is a BAD idea very often. For time-series data, it’s easier to infer gaps than it is to predict a block in the future. The latter is the more common task but a random split simulates the former, giving unrealistically good performance. For image data, there may be people, boats, etc that are in the training set but not the test set. By failing to have new people in the validation set, the model can learn things about specific people/boats that it can’t rely on in practice.\nMetrics vs loss functions: Metrics are things that are human-understandable. Loss functions should be smooth and differentiable to aid in training.\nThese can sometimes be the same thing, but not in general. For example, accuracy is a good metric in image classification. We could tweak the weights in such a way that it improves the model slightly, but not so much that it now correctly classifies a previously incorrect image. This means the metric function is bumpy, therefore a bad loss function. https://www.fast.ai/posts/2019-09-24-metrics.html\nAI can be particularly dangerous at confirming systematic biases, because it is so good at optimising metrics, so it will conform to any biases present in the training data. MAking decisions based on the model then reinforces those biases. - Goodhart’s law applies: If a metric becomes a target it’s no longer a good metric\nCorrelations The best way to understand a metric is not to look at the mathematical formular, but to plot some data for which the metric is high, medium and low, then see what that tells you.\nAfter that, look at the equation to see if your intuition matches the logic.\nChoosing a learning rate Fast AI has a function to find a good starting point. Otherwise, pick a small value and keep doubling it until it falls apart."
  },
  {
    "objectID": "posts/fastai/index.html#from-scratch-model",
    "href": "posts/fastai/index.html#from-scratch-model",
    "title": "Fast AI Notes",
    "section": "",
    "text": "Homework task: Recreate the Jupyter notebook to train a linear model and a neural network from scratch\nThen use the fast AI library to recreate the same model in less code\nRead the numpy broadcasting rules\nRead through Titanic notebooks\n\nTrain a linear model from scratch in Python using on the Titanic dataset. Then train a neural network from scratch in a similar way. This is an extension of the spreadsheet approach to make a neural network from scratch in lesson 3.\nImputing missing values Never throw away data. An easy way of imputing missing values is to fill them with the mode. This is good enough for a first pass at creating a model.\nScaling values Numeric values that can grow exponentially like prices or population sizes often have long-tailed distributions. An easy way to scale is to take log(x+1). The +1 is just to avoid taking log of 0.\nCategorical variables One-hot encode any categorical variables. We should include an “other” category for each in case the validation or test set contains a category we didn’t encounter in the training set. If there are categories with ony a small number of observations we can group them into an “other” category too.\nBroadcasting Broadcasting arrays together avoids boilerplate code to make dimensions match. https://numpy.org/doc/stable/user/basics.broadcasting.html https://tryapl.org/\nSigmoid final layer For a binary classification model, the outputs should be between 0 and 1. If you train a linear model, it might result in negative values or values &gt;1. This means we can improve the loss by just clipping to ensure they stay between 0 and 1. This is the idea behind the sigmoid function for output layers: smaller values will tend to 0 and larger values will tend to 1. In general, for any binary classification model, we should always have a sigmoid as the final layer. If the model isn’t training well, it’s worth checking that the final activation function is a sigmoid.\nFunction: y = 1 / (1+e^-x))\nFocus on input and output layers In general, the middle layers of a neural network are similar between different problems. The input layer will depend on the data for our specific problem. The output will depend on the target for our specific problem. So we spend most of our time thinking about the correct input and output layers, and the hidden layers are less important.\nUsing a framework When creating the models from scratch, there was a lot of boilerplate code to: - Impute missing values using the “obvious” method (fill with mode) - Normalise continuous variables to be between 0 and 1 - One hot encode categorical variables - Repeat all of these steps in the same order for the test set\nThe benefits of using a framework like fastai: - Less boilerplate, so the obvious things are done automatically unless you say otherwise - Repeating all of the feature engineering steps on the output is trivial with DataLoaders\nEnsemble Creating independent models and taking the mean of their predictions improves the accuracy. There are a few different approaches to ensembling a categorical variable: 1. Take the mean of the predictions (binary 0 or 1) 2. Take the mean of the probabilities (continuous between 0 and 1) then threshold the result 3. Take the mode of the predictions (binary 0 or 1)\nIn general the mean approaches work better but there’s no rule as to why, so try all of them."
  },
  {
    "objectID": "posts/fastai/index.html#random-forests",
    "href": "posts/fastai/index.html#random-forests",
    "title": "Fast AI Notes",
    "section": "",
    "text": "It’s worth starting with a random forest as a baseline model because “it’s very hard to screw up”. Decision trees only consider the ordering of data, so are not sensitive to outliers, skewed distributions, dummy variables etc.\nA random forest is an ensemble of trees. A tree is an ensemble of binary splits. binary split -&gt; tree -&gt; forest\nBinary split Pick a column of the data set and split the rows into two groups. Predict the outcome based just on that. For example, in the titanic dataset, pick the Sex column. This splits into male vs female. If we predict that all female passengers survived and all male passengers died, that’s a reasonably accurate prediction.\nA model which iterates through the variables and finds the best binary split is called a “OneR” or one rule model. This was actually the state-of-the-art in the 90s and performs reasonably well across a lot of problems.\nDecision tree This extends the “OneR” idea to two or more splits. E.g. if we first split by sex, then find the next best variable to split on to create a two layer tree.\nLeaf nodes are the number of terminating nodes in the tree. OneR creates 2 leaf nodes. If we split one side again, we’ll have 3 leaf nodes. If we split the other side, to create a balanced two layer tree, we’ll have 4 leaf nodes.\nGini is another measure of inequality, similar to the score used in the notebook to quantify how good a split was. Intuitively, this measures the likelihood that if you picked two samples from a group, how likely is it they’d be the same every time. If the group is all the same, the probability is 1. If every item is different, the probability is 0.\nRandom forest The idea behind bagging is that if we have many unbiased, uncorrelated models, we can average their predictions to get an aggregate model that is better than any of them. Each individual model will overfit, so either be too high or too low for a given point, a positive or negative error term respectively. By combining multiple uncorrelated models, the error will average to 0.\nAn easy way to get many uncorrelated models is to only use a random subset of the data each time. Then build a decision tree for each subset. This is the idea behind random forests.\nThe error decreases with the number of tree, with diminishing returns. Jeremy’s rule of thumb: improvements level off after about 30 and he doesn’t often use &gt;100.\nFeature importance A nice side effect of using decision trees is that we get feature importance plots for free.\nThe idea is for each split of a decision tree, we can track the column used to split and the amount that the gini decreased by. If we loop through the tree and accumulate the “gini reduction” for each column, we have a metric of how important that column was in splitting the dataset.\nThis makes random forests a useful first model when tackling a new big dataset, as it can give an insight into how useful each column is.\nA comment on explainability, regarding feature importance compared to SHAP, LIME etc. These explain the model, so to be useful the model needs to be accurate. If you use feature importance from a bad model then the columns it claims are important might not actually be. So the usefulness of explainability techniques boils down to what models can they explain and how accurate are those models.\nOut-of-bag error For each tree, we trained on a subset of the rows. We can then see how each one performed on the held out data; this is the out-of-bag error per tree. We can average this over all of the trees to get an overall OOB error for the forest.\nPartial dependence plots These are not specific to random forests and can be applied to any ML model including deep neural networks.\nIf we want to know how an independent variable affects the dependent variable, a naive approach would be to just plot them. But this could include a confounding variable. For example, the price of bulldozers increases over time, but driven by the presence of air conditioning which also increased over time.\nA partial dependence plot takes each row in turn and sets the column(s) of interest to the first value, say, year=1950. Then we predict the target variable using our model. Then repeat this for year=1951, 1952, etc. We can then average the target variable per year to get a view of how it depends on the independent variable, all else being equal.\nGradient boosting forests We make multiple trees, but instead of fitting all to different data subsets, we fit to residuals. So we fit a very small tree (OneR even) to the data to get a first prediction. Then we calculate the error term. Then we fit another tree to predict the error term. Then calculate the second order error term. Then fit a tree to this, etc.\nThen our prediction is the SUM of these trees (rather than the average like with a random forest). This is “boosting”; calculating an error term then fitting another model to it. Contrast this with “bagging” which was when we calculate multiple models to different subsets of data and average them.\nKaggle iterations The focus should be: 1. Create an effective validation set 2. Iterate quickly to find changes which improve the validation set\nTrain a simple model straight away, get a result and submit it. You can iterate and improve later.\nRules of thumb on model selection by problem type: - For computer vision use CNNs, fine-tune a pre-trained model. See comparison of pre-trained models here - If unsure which model to choose, start with (small) convnext. - Different models can have big differences in accuracy so try a few small models from different families. - Once you are happy with the model, try a size up in that family. - For tabular data, random forests will give a reasonably good results. - GBMs will give a better result eventually, but with more effort required to get a good model. - Worth running a hyperparameter grid search for GBM because it’s fiddly.\nRules of thumb on hardware: - You generally want ~8 physical CPUs per GPU - If model training is CPU bound, it can help to resize images to be smaller. The file I/O on the CPU may be taking too long. - If model training is CPU bound and GPU usage is low, you might as well go up to a “better” (i.e. bigger) model with no increase of overall execution time.\nData augmentation, in particular test-time augmentation (TTA), can improve results.\nResizing images to a square is a good, easy compromise to accomodate many different iamge sizes , orientations and aspect ratios. A more involved approach that may improve results is to batch images together and resize them to the median rectangle size."
  },
  {
    "objectID": "posts/fastai/index.html#further-reading",
    "href": "posts/fastai/index.html#further-reading",
    "title": "Fast AI Notes",
    "section": "",
    "text": "Bagging predictors https://link.springer.com/article/10.1007/BF00058655\nStatistical Modeling: The Two Cultures https://www.semanticscholar.org/paper/Statistical-Modeling%3A-The-Two-Cultures-(with-and-a-Breiman/e5df6bc6da5653ad98e754b08f63326c2e52b372\nComparison of vision models https://www.kaggle.com/code/jhoward/the-best-vision-models-for-fine-tuning"
  },
  {
    "objectID": "posts/fastai/index.html#collaborative-filtering",
    "href": "posts/fastai/index.html#collaborative-filtering",
    "title": "Fast AI Notes",
    "section": "7. Collaborative filtering",
    "text": "7. Collaborative filtering"
  }
]